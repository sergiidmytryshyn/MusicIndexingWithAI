"""
Benchmark script for testing field extraction from prompts generated by generate_prompts.py.
Reads prompts from generated_prompts.json, runs extraction, generates a report, and updates the original JSON.
"""

import json
from typing import Dict, List
from datetime import datetime
import sys
import os

# Add parent directory to path to import from app.py
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Import extraction function from app.py
# We need to mock streamlit since it's not available in CLI context
import streamlit as st
st.write = lambda *args, **kwargs: None  # Mock st.write to do nothing
st.error = lambda *args, **kwargs: None
st.warning = lambda *args, **kwargs: None
st.code = lambda *args, **kwargs: None
st.stop = lambda: None

from app import extract_fields_with_costar

# Configuration
GENERATED_PROMPTS_FILE = "data_parsing/data/generated_prompts.json"


def run_benchmark(prompts_file: str = GENERATED_PROMPTS_FILE):
    """Run benchmark tests on prompts from generated_prompts.json and update the file."""
    
    # Load generated prompts
    print(f"Loading prompts from: {prompts_file}")
    try:
        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompts_data = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå Error: File {prompts_file} not found!")
        return None
    
    prompts_list = prompts_data.get('prompts', [])
    print(f"Found {len(prompts_list)} prompts\n")
    
    if not prompts_list:
        print("‚ùå No prompts found in file!")
        return None
    
    results = []
    
    print(f"Running benchmark on {len(prompts_list)} prompts...")
    print("=" * 80)
    
    # Process each prompt
    for i, prompt_entry in enumerate(prompts_list, 1):
        generated_prompt = prompt_entry.get('generated_prompt', '')
        track_title = prompt_entry.get('title', 'Unknown')
        artist_name = prompt_entry.get('artist_name', 'Unknown')
        
        print(f"\n[{i}/{len(prompts_list)}] Processing: {track_title} - {artist_name}")
        print(f"  Prompt: {generated_prompt[:80]}...")
        
        if not generated_prompt:
            print(f"  ‚ö†Ô∏è  No prompt found, skipping...")
            continue
        
        try:
            # Extract fields from the prompt
            result = extract_fields_with_costar(generated_prompt)
            
            # Extract debug info
            debug_info = result.pop("_debug", {})
            
            # Add extracted JSON to the prompt entry
            prompt_entry['extracted_json'] = result
            prompt_entry['extraction_debug'] = {
                "parsed_fields": debug_info.get("parsed_fields", []),
                "missing_fields": debug_info.get("missing_fields", []),
                "parsing_errors": debug_info.get("parsing_errors", []),
                "raw_xml": debug_info.get("raw_xml", "N/A"),
                "has_errors": len(debug_info.get("parsing_errors", [])) > 0 or len(debug_info.get("missing_fields", [])) > 0
            }
            
            # Create result entry for report
            entry = {
                "track_id": prompt_entry.get("track_id"),
                "title": track_title,
                "artist_name": artist_name,
                "prompt": generated_prompt,
                "raw_xml": debug_info.get("raw_xml", "N/A"),
                "extracted_json": result,
                "parsed_fields": debug_info.get("parsed_fields", []),
                "missing_fields": debug_info.get("missing_fields", []),
                "parsing_errors": debug_info.get("parsing_errors", []),
                "has_errors": prompt_entry['extraction_debug']["has_errors"]
            }
            
            results.append(entry)
            
            # Print summary
            if entry["has_errors"]:
                print(f"  ‚ö†Ô∏è  Issues found: {len(entry['parsing_errors'])} errors, {len(entry['missing_fields'])} missing fields")
            else:
                print(f"  ‚úÖ Successfully extracted")
                
        except Exception as e:
            print(f"  ‚ùå Failed: {str(e)}")
            # Add error info to prompt entry
            prompt_entry['extracted_json'] = {}
            prompt_entry['extraction_debug'] = {
                "parsed_fields": [],
                "missing_fields": ["Benchmark execution failed"],
                "parsing_errors": [f"Exception: {str(e)}"],
                "raw_xml": "N/A",
                "has_errors": True
            }
            
            results.append({
                "track_id": prompt_entry.get("track_id"),
                "title": track_title,
                "artist_name": artist_name,
                "prompt": generated_prompt,
                "raw_xml": "N/A",
                "extracted_json": {},
                "parsed_fields": [],
                "missing_fields": ["Benchmark execution failed"],
                "parsing_errors": [f"Exception: {str(e)}"],
                "has_errors": True
            })
    
    # Update the original prompts file with extracted JSON
    print(f"\n{'=' * 80}")
    print(f"Updating {prompts_file} with extracted JSON...")
    try:
        with open(prompts_file, 'w', encoding='utf-8') as f:
            json.dump(prompts_data, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Updated {prompts_file}")
    except Exception as e:
        print(f"‚ùå Error updating prompts file: {e}")
    
    # Generate report file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"benchmark_report_{timestamp}.json"
    
    report = {
        "timestamp": timestamp,
        "source_file": prompts_file,
        "total_prompts": len(prompts_list),
        "results": results,
        "summary": {
            "total_tests": len(results),
            "successful": len([r for r in results if not r["has_errors"]]),
            "with_errors": len([r for r in results if r["has_errors"]]),
            "total_parsing_errors": sum(len(r["parsing_errors"]) for r in results),
            "total_missing_fields": sum(len(r["missing_fields"]) for r in results)
        }
    }
    
    # Save report
    with open(report_filename, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'=' * 80}")
    print(f"\n‚úÖ Benchmark complete!")
    print(f"üìä Summary:")
    print(f"   Total tests: {report['summary']['total_tests']}")
    print(f"   Successful: {report['summary']['successful']}")
    print(f"   With errors: {report['summary']['with_errors']}")
    print(f"   Total parsing errors: {report['summary']['total_parsing_errors']}")
    print(f"   Total missing fields: {report['summary']['total_missing_fields']}")
    print(f"\nüìÑ Report saved to: {report_filename}")
    print(f"üìÑ Updated prompts file: {prompts_file}")
    
    return report_filename


if __name__ == "__main__":
    run_benchmark()

